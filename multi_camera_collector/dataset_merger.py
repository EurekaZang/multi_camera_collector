#!/usr/bin/env python3
"""
å¤šç›¸æœºæ•°æ®é›†åˆå¹¶å·¥å…·

æ­¤æ¨¡å—è´Ÿè´£åˆå¹¶å¤šä¸ªç”±multi_camera_collectorç”Ÿæˆçš„æ•°æ®é›†ï¼Œ
å¹¶ç”Ÿæˆç»Ÿä¸€çš„CSVç´¢å¼•æ–‡ä»¶ã€‚
"""

import os
import shutil
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional
import argparse
import sys
from datetime import datetime


class DatasetMerger:
    """æ•°æ®é›†åˆå¹¶å™¨"""
    
    def __init__(self, output_path: str):
        """
        åˆå§‹åŒ–æ•°æ®é›†åˆå¹¶å™¨
        
        Args:
            output_path (str): åˆå¹¶åæ•°æ®é›†çš„è¾“å‡ºè·¯å¾„
        """
        self.output_path = Path(output_path).resolve()
        self.camera_types = ['camera', 'camera_femto']
        self.source_datasets: List[Path] = []
        
        print(f"åˆå§‹åŒ–æ•°æ®é›†åˆå¹¶å™¨ï¼Œè¾“å‡ºè·¯å¾„: {self.output_path}")
    
    def add_dataset(self, dataset_path: str) -> bool:
        """
        æ·»åŠ ä¸€ä¸ªæ•°æ®é›†åˆ°åˆå¹¶åˆ—è¡¨
        
        Args:
            dataset_path (str): æ•°æ®é›†è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        path = Path(dataset_path).resolve()
        
        # éªŒè¯æ•°æ®é›†è·¯å¾„
        if not path.exists():
            print(f"âš ï¸  è­¦å‘Š: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {path}")
            return False
        
        if not path.is_dir():
            print(f"âš ï¸  è­¦å‘Š: è·¯å¾„ä¸æ˜¯ç›®å½•: {path}")
            return False
        
        # éªŒè¯æ•°æ®é›†ç»“æ„
        valid = False
        for camera_type in self.camera_types:
            camera_path = path / camera_type
            if camera_path.exists():
                required_dirs = ['rgb', 'depth', 'camera_info']
                if all((camera_path / d).exists() for d in required_dirs):
                    valid = True
                    break
        
        if not valid:
            print(f"âš ï¸  è­¦å‘Š: æ•°æ®é›†ç»“æ„ä¸æ­£ç¡®: {path}")
            return False
        
        self.source_datasets.append(path)
        print(f"âœ“ å·²æ·»åŠ æ•°æ®é›†: {path}")
        return True
    
    def _create_output_structure(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        print("\nğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„...")
        
        for camera_type in self.camera_types:
            camera_path = self.output_path / camera_type
            (camera_path / 'rgb').mkdir(parents=True, exist_ok=True)
            (camera_path / 'depth').mkdir(parents=True, exist_ok=True)
            (camera_path / 'camera_info').mkdir(parents=True, exist_ok=True)
        
        print("âœ“ è¾“å‡ºç›®å½•ç»“æ„å·²åˆ›å»º")
    
    def _extract_timestamp_from_filename(self, filename: str) -> Optional[int]:
        """
        ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³
        
        Args:
            filename (str): æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
            
        Returns:
            Optional[int]: æ—¶é—´æˆ³ï¼ˆçº³ç§’ï¼‰ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        try:
            # ç§»é™¤æ–‡ä»¶æ‰©å±•åå’Œå¯èƒ½çš„åç¼€
            name_without_ext = filename.split('.')[0]
            
            # å¤„ç†camera_infoæ–‡ä»¶çš„ç‰¹æ®Šå‘½åæ ¼å¼
            if '_rgb_camera_info' in name_without_ext:
                timestamp_str = name_without_ext.replace('_rgb_camera_info', '')
            elif '_depth_camera_info' in name_without_ext:
                timestamp_str = name_without_ext.replace('_depth_camera_info', '')
            else:
                timestamp_str = name_without_ext
            
            return int(timestamp_str)
        except ValueError:
            return None
    
    def _get_dataset_info(self, dataset_path: Path) -> Dict:
        """
        è·å–æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯
        
        Args:
            dataset_path (Path): æ•°æ®é›†è·¯å¾„
            
        Returns:
            Dict: æ•°æ®é›†ä¿¡æ¯
        """
        info = {
            'path': str(dataset_path),
            'cameras': {}
        }
        
        for camera_type in self.camera_types:
            camera_path = dataset_path / camera_type
            if not camera_path.exists():
                continue
            
            rgb_path = camera_path / 'rgb'
            if rgb_path.exists():
                rgb_files = list(rgb_path.glob('*.png'))
                timestamps = []
                for rgb_file in rgb_files:
                    ts = self._extract_timestamp_from_filename(rgb_file.name)
                    if ts is not None:
                        timestamps.append(ts)
                
                if timestamps:
                    info['cameras'][camera_type] = {
                        'count': len(timestamps),
                        'min_timestamp': min(timestamps),
                        'max_timestamp': max(timestamps)
                    }
        
        return info
    
    def _check_timestamp_conflicts(self) -> Dict[str, List[tuple]]:
        """
        æ£€æŸ¥æ•°æ®é›†ä¹‹é—´çš„æ—¶é—´æˆ³å†²çª
        
        Returns:
            Dict[str, List[tuple]]: æ¯ä¸ªç›¸æœºç±»å‹çš„å†²çªåˆ—è¡¨ (timestamp, dataset_index1, dataset_index2)
        """
        conflicts = {camera_type: [] for camera_type in self.camera_types}
        
        for camera_type in self.camera_types:
            timestamp_to_dataset = {}
            
            for idx, dataset_path in enumerate(self.source_datasets):
                camera_path = dataset_path / camera_type / 'rgb'
                if not camera_path.exists():
                    continue
                
                for rgb_file in camera_path.glob('*.png'):
                    ts = self._extract_timestamp_from_filename(rgb_file.name)
                    if ts is not None:
                        if ts in timestamp_to_dataset:
                            conflicts[camera_type].append((ts, timestamp_to_dataset[ts], idx))
                        else:
                            timestamp_to_dataset[ts] = idx
        
        return conflicts
    
    def _copy_dataset_files(self, dataset_path: Path, dataset_index: int, 
                           timestamp_offset: int = 0) -> Dict[str, int]:
        """
        å¤åˆ¶ä¸€ä¸ªæ•°æ®é›†çš„æ‰€æœ‰æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
        
        Args:
            dataset_path (Path): æºæ•°æ®é›†è·¯å¾„
            dataset_index (int): æ•°æ®é›†ç´¢å¼•
            timestamp_offset (int): æ—¶é—´æˆ³åç§»é‡ï¼ˆç”¨äºè§£å†³å†²çªï¼‰
            
        Returns:
            Dict[str, int]: æ¯ä¸ªç›¸æœºç±»å‹å¤åˆ¶çš„æ–‡ä»¶æ•°é‡
        """
        copied_counts = {}
        
        for camera_type in self.camera_types:
            src_camera_path = dataset_path / camera_type
            if not src_camera_path.exists():
                continue
            
            dst_camera_path = self.output_path / camera_type
            count = 0
            
            # è·å–æ‰€æœ‰æ—¶é—´æˆ³
            rgb_path = src_camera_path / 'rgb'
            if not rgb_path.exists():
                continue
            
            rgb_files = list(rgb_path.glob('*.png'))
            
            for rgb_file in rgb_files:
                timestamp = self._extract_timestamp_from_filename(rgb_file.name)
                if timestamp is None:
                    continue
                
                # åº”ç”¨æ—¶é—´æˆ³åç§»
                new_timestamp = timestamp + timestamp_offset
                
                # å®šä¹‰æ–‡ä»¶è·¯å¾„
                files_to_copy = [
                    (src_camera_path / 'rgb' / f"{timestamp}.png",
                     dst_camera_path / 'rgb' / f"{new_timestamp}.png"),
                    (src_camera_path / 'depth' / f"{timestamp}.png",
                     dst_camera_path / 'depth' / f"{new_timestamp}.png"),
                    (src_camera_path / 'camera_info' / f"{timestamp}_rgb_camera_info.json",
                     dst_camera_path / 'camera_info' / f"{new_timestamp}_rgb_camera_info.json"),
                    (src_camera_path / 'camera_info' / f"{timestamp}_depth_camera_info.json",
                     dst_camera_path / 'camera_info' / f"{new_timestamp}_depth_camera_info.json"),
                ]
                
                # æ£€æŸ¥æ‰€æœ‰æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if all(src.exists() for src, _ in files_to_copy):
                    # å¤åˆ¶æ‰€æœ‰æ–‡ä»¶
                    for src, dst in files_to_copy:
                        shutil.copy2(src, dst)
                    count += 1
            
            copied_counts[camera_type] = count
        
        return copied_counts
    
    def merge(self, resolve_conflicts: str = 'offset') -> bool:
        """
        æ‰§è¡Œæ•°æ®é›†åˆå¹¶æ“ä½œ
        
        Args:
            resolve_conflicts (str): å†²çªè§£å†³ç­–ç•¥
                - 'offset': ä¸ºå†²çªçš„æ—¶é—´æˆ³æ·»åŠ åç§»é‡
                - 'skip': è·³è¿‡å†²çªçš„æ–‡ä»¶
                - 'overwrite': è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
                
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆå¹¶
        """
        if not self.source_datasets:
            print("âŒ é”™è¯¯: æ²¡æœ‰æ·»åŠ ä»»ä½•æ•°æ®é›†")
            return False
        
        print(f"\nğŸ”„ å¼€å§‹åˆå¹¶ {len(self.source_datasets)} ä¸ªæ•°æ®é›†...")
        print(f"å†²çªè§£å†³ç­–ç•¥: {resolve_conflicts}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self._create_output_structure()
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        print("\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        for idx, dataset_path in enumerate(self.source_datasets):
            info = self._get_dataset_info(dataset_path)
            print(f"\næ•°æ®é›† {idx + 1}: {dataset_path.name}")
            for camera_type, camera_info in info['cameras'].items():
                duration = (camera_info['max_timestamp'] - camera_info['min_timestamp']) / 1e9
                print(f"  {camera_type}: {camera_info['count']} ç»„ï¼Œæ—¶é•¿ {duration:.1f}s")
        
        # æ£€æŸ¥æ—¶é—´æˆ³å†²çª
        if resolve_conflicts == 'offset':
            print("\nğŸ” æ£€æŸ¥æ—¶é—´æˆ³å†²çª...")
            conflicts = self._check_timestamp_conflicts()
            
            total_conflicts = sum(len(c) for c in conflicts.values())
            if total_conflicts > 0:
                print(f"âš ï¸  å‘ç° {total_conflicts} ä¸ªæ—¶é—´æˆ³å†²çª")
                for camera_type, conflict_list in conflicts.items():
                    if conflict_list:
                        print(f"  {camera_type}: {len(conflict_list)} ä¸ªå†²çª")
                print("å°†ä½¿ç”¨æ—¶é—´æˆ³åç§»ç­–ç•¥è§£å†³å†²çª...")
            else:
                print("âœ“ æœªå‘ç°æ—¶é—´æˆ³å†²çª")
        
        # å¤åˆ¶æ–‡ä»¶
        print("\nğŸ“¦ å¤åˆ¶æ–‡ä»¶...")
        total_copied = {camera_type: 0 for camera_type in self.camera_types}
        
        timestamp_offset = 0
        for idx, dataset_path in enumerate(self.source_datasets):
            print(f"\nå¤„ç†æ•°æ®é›† {idx + 1}/{len(self.source_datasets)}: {dataset_path.name}")
            
            # è®¡ç®—æ—¶é—´æˆ³åç§»
            if resolve_conflicts == 'offset' and idx > 0:
                # ä½¿ç”¨å‰ä¸€ä¸ªæ•°æ®é›†çš„æœ€å¤§æ—¶é—´æˆ³ä½œä¸ºåç§»åŸºå‡†
                info = self._get_dataset_info(self.source_datasets[idx - 1])
                max_timestamp = 0
                for camera_info in info['cameras'].values():
                    max_timestamp = max(max_timestamp, camera_info['max_timestamp'])
                
                # æ·»åŠ 1ç§’çš„ç¼“å†²åŒº
                timestamp_offset = max_timestamp + int(1e9)
            
            copied_counts = self._copy_dataset_files(dataset_path, idx, timestamp_offset)
            
            for camera_type, count in copied_counts.items():
                total_copied[camera_type] += count
                if count > 0:
                    print(f"  âœ“ {camera_type}: {count} ç»„")
        
        print(f"\nâœ“ æ–‡ä»¶å¤åˆ¶å®Œæˆ")
        print("æ€»è®¡:")
        for camera_type, count in total_copied.items():
            if count > 0:
                print(f"  {camera_type}: {count} ç»„")
        
        return True
    
    def generate_csv(self, output_format: str = 'combined') -> List[str]:
        """
        ä¸ºåˆå¹¶åçš„æ•°æ®é›†ç”ŸæˆCSVæ–‡ä»¶
        
        Args:
            output_format (str): è¾“å‡ºæ ¼å¼
                - 'combined': ç”Ÿæˆå•ä¸ªåŒ…å«æ‰€æœ‰ç›¸æœºæ•°æ®çš„CSVæ–‡ä»¶
                - 'separate': ä¸ºæ¯ä¸ªç›¸æœºç”Ÿæˆå•ç‹¬çš„CSVæ–‡ä»¶
                - 'both': åŒæ—¶ç”Ÿæˆç»„åˆå’Œå•ç‹¬çš„CSVæ–‡ä»¶
                
        Returns:
            List[str]: ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        print(f"\nğŸ“Š ç”ŸæˆCSVæ–‡ä»¶...")
        
        # ä½¿ç”¨DatasetGeneratorç”ŸæˆCSV
        try:
            # åŠ¨æ€å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
            from multi_camera_collector.dataset_generator import DatasetGenerator
        except ImportError:
            try:
                current_dir = Path(__file__).parent
                if str(current_dir) not in sys.path:
                    sys.path.insert(0, str(current_dir))
                from dataset_generator import DatasetGenerator
            except ImportError:
                print("âŒ é”™è¯¯: æ— æ³•å¯¼å…¥dataset_generatoræ¨¡å—")
                return []
        
        try:
            generator = DatasetGenerator(str(self.output_path))
            generated_files = generator.generate_csv_files(output_format)
            
            if generated_files:
                print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªCSVæ–‡ä»¶")
                for file_path in generated_files:
                    file_size = Path(file_path).stat().st_size
                    print(f"  ğŸ“„ {Path(file_path).name} ({file_size:,} bytes)")
            
            return generated_files
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆCSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def save_merge_info(self):
        """ä¿å­˜åˆå¹¶ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        merge_info = {
            'merge_date': datetime.now().isoformat(),
            'output_path': str(self.output_path),
            'source_datasets': [str(p) for p in self.source_datasets],
            'num_datasets': len(self.source_datasets)
        }
        
        info_path = self.output_path / "merge_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(merge_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ åˆå¹¶ä¿¡æ¯å·²ä¿å­˜: {info_path}")


def main():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""
    print("ğŸ”€ å¤šç›¸æœºæ•°æ®é›†åˆå¹¶å·¥å…·")
    print("=" * 50)
    
    parser = argparse.ArgumentParser(
        description='åˆå¹¶å¤šä¸ªmulti_camera_collectoræ•°æ®é›†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†
  ./dataset_merger.py --output merged_dataset dataset1/ dataset2/
  
  # åˆå¹¶å¤šä¸ªæ•°æ®é›†å¹¶ç”Ÿæˆå•ç‹¬çš„CSVæ–‡ä»¶
  ./dataset_merger.py --output merged_dataset --format separate dataset1/ dataset2/ dataset3/
  
  # ä½¿ç”¨ä¸åŒçš„å†²çªè§£å†³ç­–ç•¥
  ./dataset_merger.py --output merged_dataset --conflicts overwrite dataset1/ dataset2/

æ•°æ®é›†ç»“æ„è¦æ±‚:
  æ¯ä¸ªè¾“å…¥æ•°æ®é›†åº”åŒ…å«:
  â”œâ”€â”€ camera/
  â”‚   â”œâ”€â”€ rgb/           # æ ‡å‡†ç›¸æœºRGBå›¾åƒ
  â”‚   â”œâ”€â”€ depth/         # æ ‡å‡†ç›¸æœºæ·±åº¦å›¾åƒ
  â”‚   â””â”€â”€ camera_info/   # æ ‡å‡†ç›¸æœºå‚æ•°æ–‡ä»¶
  â””â”€â”€ camera_femto/
      â”œâ”€â”€ rgb/           # Femtoç›¸æœºRGBå›¾åƒ
      â”œâ”€â”€ depth/         # Femtoç›¸æœºæ·±åº¦å›¾åƒ
      â””â”€â”€ camera_info/   # Femtoç›¸æœºå‚æ•°æ–‡ä»¶

è¾“å‡º:
  åˆå¹¶åçš„æ•°æ®é›†å°†åŒ…å«æ‰€æœ‰è¾“å…¥æ•°æ®é›†çš„æ•°æ®ï¼Œ
  å¹¶ç”Ÿæˆç»Ÿä¸€çš„CSVç´¢å¼•æ–‡ä»¶ï¼Œæ ¼å¼ä¸make_dataset.pyç”Ÿæˆçš„ç›¸åŒã€‚
        """
    )
    
    parser.add_argument(
        'datasets',
        nargs='+',
        help='è¦åˆå¹¶çš„æ•°æ®é›†è·¯å¾„ï¼ˆè‡³å°‘2ä¸ªï¼‰'
    )
    
    parser.add_argument(
        '--output', '-o',
        required=True,
        help='åˆå¹¶åæ•°æ®é›†çš„è¾“å‡ºè·¯å¾„'
    )
    
    parser.add_argument(
        '--format', '-f',
        choices=['combined', 'separate', 'both'],
        default='combined',
        help='CSVæ–‡ä»¶è¾“å‡ºæ ¼å¼ (é»˜è®¤: combined)'
    )
    
    parser.add_argument(
        '--conflicts', '-c',
        choices=['offset', 'skip', 'overwrite'],
        default='offset',
        help='æ—¶é—´æˆ³å†²çªè§£å†³ç­–ç•¥ (é»˜è®¤: offset)\n'
             '  offset: ä¸ºå†²çªçš„æ—¶é—´æˆ³æ·»åŠ åç§»é‡\n'
             '  skip: è·³è¿‡å†²çªçš„æ–‡ä»¶\n'
             '  overwrite: è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºä¿¡æ¯'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥
    if len(args.datasets) < 2:
        print("âŒ é”™è¯¯: è‡³å°‘éœ€è¦2ä¸ªæ•°æ®é›†è¿›è¡Œåˆå¹¶")
        sys.exit(1)
    
    # æ£€æŸ¥è¾“å‡ºè·¯å¾„
    output_path = Path(args.output).resolve()
    if output_path.exists():
        print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºè·¯å¾„å·²å­˜åœ¨: {output_path}")
        response = input("æ˜¯å¦è¦è¦†ç›–? (y/N): ")
        if response.lower() != 'y':
            print("æ“ä½œå·²å–æ¶ˆ")
            sys.exit(0)
        print("å°†è¦†ç›–ç°æœ‰å†…å®¹...")
    
    print(f"\nğŸ“‚ è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"ğŸ“Š CSVæ ¼å¼: {args.format}")
    print(f"âš™ï¸  å†²çªç­–ç•¥: {args.conflicts}")
    print()
    
    try:
        # åˆ›å»ºåˆå¹¶å™¨
        merger = DatasetMerger(str(output_path))
        
        # æ·»åŠ æ•°æ®é›†
        print("ğŸ“‹ æ·»åŠ æ•°æ®é›†:")
        for dataset_path in args.datasets:
            if not merger.add_dataset(dataset_path):
                print(f"âš ï¸  è·³è¿‡æ— æ•ˆæ•°æ®é›†: {dataset_path}")
        
        if len(merger.source_datasets) < 2:
            print("\nâŒ é”™è¯¯: è‡³å°‘éœ€è¦2ä¸ªæœ‰æ•ˆæ•°æ®é›†")
            sys.exit(1)
        
        # æ‰§è¡Œåˆå¹¶
        if not merger.merge(resolve_conflicts=args.conflicts):
            print("\nâŒ æ•°æ®é›†åˆå¹¶å¤±è´¥")
            sys.exit(1)
        
        # ç”ŸæˆCSVæ–‡ä»¶
        generated_files = merger.generate_csv(args.format)
        
        if not generated_files:
            print("\nâš ï¸  è­¦å‘Š: æœªèƒ½ç”ŸæˆCSVæ–‡ä»¶")
        
        # ä¿å­˜åˆå¹¶ä¿¡æ¯
        merger.save_merge_info()
        
        print("\nğŸ‰ æ•°æ®é›†åˆå¹¶å®Œæˆï¼")
        print(f"âœ… è¾“å‡ºä½ç½®: {output_path}")
        
        if generated_files:
            print(f"\nğŸ’¡ ä½¿ç”¨æç¤º:")
            print(f"  â€¢ åˆå¹¶åçš„æ•°æ®é›†å¯ç›´æ¥ç”¨äºæ·±åº¦å­¦ä¹ è®­ç»ƒ")
            print(f"  â€¢ CSVæ–‡ä»¶æ ¼å¼ä¸make_dataset.pyç”Ÿæˆçš„ç›¸åŒ")
            print(f"  â€¢ æŸ¥çœ‹ merge_info.json äº†è§£åˆå¹¶è¯¦æƒ…")
            print(f"  â€¢ æŸ¥çœ‹ dataset_statistics.json äº†è§£æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
